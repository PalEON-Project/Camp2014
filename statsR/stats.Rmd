# PalEON Summer Course: Statistics Review
## August 2014
## Chris Paciorek

In this module, we’ll review the basics of probability, and touch back on some programming in R. Much
of the material will be presented on the board, so some of these sections are blank or incomplete.

### Review of probability
Let’s review probability by working through a canonical use of Bayes theorem in diagnostic testing.
In particular we want to think about

$$
\begin{aligned}
  P(A|B)  = \frac{P(B|A)P(A)}{P(B)} 
\end{aligned}
$$ 
 
but for concreteness let’s do it with notation that is evocative and use some numbers.

**Example**
Suppose someone is tested for a rare disease that occurs in the population in 1 in 1000 people
and there is no information that suggests the person has the disease. It’s just a routine test. Also suppose the test is quite accurate: the probability of a false negative is .01 and the probability of a false positive is .01.

$$
P(D)     = .001 
$$
$$
P(−|D)   = .01 
$$
$$
P(+|D)   = .99 
$$
$$
P(+|D^c) = .01 
$$

You are the person. Your doctor calls you in and tells you that you tested positive. Should you be worried? How worried? 

Let’s work through this and illustrate with a Venn diagram.

**Questions**

1.  Suppose $A$ and $B$ are independent. What is $P(A|B)$?
2.  Suppose $A$ and $B$ are mutually exclusive ($A$ can’t happen if $B$ happens). What is $P(A|B)$?

### Interpreting Bayes’ theorem in the context of Bayesian statistics

Standard (frequentist) statistics that you are used to, such as working with p-values, ask questions such as:
If the null hypothesis is true, how likely/extreme are the data, e.g.: $P(Y|\theta= 0)$.

Bayesian statistics uses Bayes theorem to turn the conditioning around:

$$
\begin{aligned}
P(\theta|Y) &=       \frac{P(Y|\theta)P(\theta)}{P(Y)} \\
       &\propto P(Y|\theta)P(\theta)
\end{aligned}
$$

Given the data you’ve seen, what can we say about $\theta$? In this case the data are just an event $Y$ - did it happen or not. And $\theta$ is just a 0/1 state of the world that we want to know. Think of two hypotheses that we are deciding between. Once you see the data, what is the probability of one of the hypotheses.
When we get to Bayesian statistics, the formula will still hold, but $Y$ and $\theta$ will be random variables
instead of events.

**Challenge:** In the context of the diagnostic testing example, consider the likelihood and the prior. Where
would we would get numerical estimates of these quantities so we could apply Bayes theorem?
  
### Discrete random variables
Let’s use the example of coin-flipping to illustrate discrete random variables. We’ll talk about the probability mass function (analogous to the density), the distribution function, expected values, and variance. We’ll
cover this on the board.
Another commonly-arising discrete distribution is the Poisson distribution. The probability mass function is
$$
\begin{aligned}
P(Y=y) = \frac{\exp(-\lambda) \lambda^y}{y!}
\end{aligned}
$$
with $E(Y)= \lambda$ and $Var(Y) = \lambda$.

### Continuous random variables

We’ll use the normal distribution to illustrate continuous distributions. We’ll cover this on the board. Other commonly encountered continuous distributions are the $t$ distribution, the beta, the gamma, and the inverse gamma.

### Working with distributions in R

R has functionality for lots of calculations with a variety of widely-used distributions. The basic ones are
`rnorm()`, `dnorm()`, `pnorm()` and `qnorm()` for the normal distribution and analogs for other distribu-
tions, e.g., `rbinom()`, `dbinom()`, `pbinom()`, `qbinom()`.

```{r distributions}
y <- seq(-3, 3, length = 300)
par(mfrow = c(1, 1))
plot(y, dnorm(y), type = "l")

pnorm(-2)

pnorm(-1.96)

qnorm(0.975)

n <- 1e+06
y <- rnorm(n)

# empirical analogs of the distributional quantities above
par(mfrow = c(1, 2))
hist(y)
hist(y, probability = TRUE)

mean(y)

mean(y <= -2)
# sum(y <= 2)/n

sort(y)[round(0.975 * n)]
# empirical 97.5th percentile

```

Note that if $n$ is large enough, we can approximate characteristics of the distribution using a sample.
We’ll see this again when we talk about MCMC.

### Joint distributions, marginal distributions and conditional distributions

We’ll cover this on the board.

### R programming

In addition to the R basics we’ve covered, we’ll also need to be able to do a bit of programming in R. The
three basic tools are functions (which we’ve seen), branching (i.e., if-else statements) and looping.

```{r if else}
x = 4
y = 7
if (x > 3 && y < 5) {
print("success")
print("doing some stuff")
} else {
4
print("failure")
print("doing something different")
}
```

Programming languages generally provide syntax for looping a fixed number of times or looping until
some condition is met. Most looping in R uses a for loop. As I’ve mentioned, loops in R are much slower
than looping in a compiled language such as C, so we try to avoid them using vectorized operations and
`apply()`. Here’s an example of a random walk (there are ways to do this without using a loop).

```{r random walk for loop}
nSteps <- 100
track <- matrix(NA, nr = nSteps, nc = 2)
track[1, ] <- c(0, 0)
bound <- 20
plot(track[1, 1], track[1, 2], xlim = c(-bound, bound), ylim =
c(-bound,
bound), pch = 16, xlab = "x", ylab = "y")
for (it in 2:nSteps) {
track[it, ] <- track[it - 1, ] + sample(c(-1, 1), 2, replace =
TRUE)
arrows(track[it - 1, 1], track[it - 1, 2], track[it, 1], track[it,
2], length = 0.05,
angle = 20)
}
```

**Challenge:** Modify the code to make the line a different color if it represents doubling-back on the most
recent movement.
  
**Challenge:** Write a random walk function that allows the user to specify the initial position and the
number of steps.


R also has a while loop that allows one to keep looping until a condition is met.
 
```{r random walk while loop}
maxSteps <- 1000
track <- matrix(NA, nr = maxSteps, nc = 2)
track[1, ] <- c(0, 0)
bound <- 20
plot(track[1, 1], track[1, 2], xlim = c(-bound, bound), ylim =
c(-bound, bound), pch = 16, xlab = "x", ylab = "y")
it <- 1
while (max(abs(track[it, ])) < bound && it < maxSteps) {
it <- it + 1
track[it, ] <- track[it - 1, ] + sample(c(-1, 1), 2, replace =
TRUE)
arrows(track[it - 1, 1], track[it - 1, 2], track[it, 1], track[it,
2], length = 0.05,
angle = 20)
}
title(main = "Computational modern art in R")
```