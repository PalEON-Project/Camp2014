# PalEON Summer Course: MCMC with JAGS module
## August 2014
## Chris Paciorek


In this module, we'll do some hands-on work to fit a time series model via MCMC with JAGS. 

Recall our mixed model (rat pups) and HMM. 
Let's work through fitting such models using
JAGS (this could also be done in BUGS).


Here we'll use simulated data.

```{r mixedModelData}
theta <- 2
sigma <- 2
nMoms <- 10

## IGNORE THE MAN BEHIND THE CURTAIN ####
set.seed(0)
mus <- rgamma(nMoms, shape = 2, scale = 1)
nPups <- rbinom(nMoms, 15, prob = .4) 
momIds <- rep(1:nMoms, times = nPups)
y <- rnorm(sum(nPups), mus[momIds], sigma)
n <- sum(nPups)
## IGNORE THE CODE ABOVE
```

Here's the JAGS code for the model. Note that the normal distribution in JAGS is in terms of 
the precision (one over the variance), so our code uses `dnorm()` differently than we do in R 
(which is in terms of the standard deviation).

```{r bugsCode_mixedModel}
### model function (note this looks like R code but is really JAGS code for defining a model
mixedModel <- function(){
  # (hyper)priors
  sigma ~ dunif(0, 100)
  tau ~ dunif(0, 100)
  theta ~ dnorm(0,.00001)
  # some transformations to get precisions from sd's
  tau2Inv <- 1/(tau^2)
  sigma2Inv <- 1/(sigma^2)
  # latent value layer
  for(i in 1:nMoms){
    mus[i] ~ dnorm(theta, tau2Inv)
  }
  # likelihood
  for(j in 1:n)
    {
      y[j] ~ dnorm(mus[momIds[j]], sigma2Inv)
    }
}
```

Here's how we run it from R.

```{r run_mixed}
library(R2jags, quietly = TRUE) 
out <- jags(data = list(y = y, n = n, nMoms = nMoms, momIds = momIds),
  parameters.to.save = c('theta','sigma','tau', 'mus'), n.chains = 1,
  n.iter = 2000, n.burnin = 1000, model.file = mixedModel, DIC = FALSE)
```


A few things to check to see if the MCMC is performing well:
- Look at the trace plots and see if the initial part of the chain looks
similar to the later part. If not, you need a longer burn-in period.
- Look at the trace plots and see if the chain is mixing. We want to
see rapid movement up and down, not slow excursions. Calculate effective sample sizes.
- See if there are any posterior correlations between parameters that
are near 1 or -1, which indicate trading off between parameters.
- Run the model with multiple chains (with different starting values)
and see that all the chains converge to the same posterior distribution.
\end{enumerate}

Let's look at some figures that illustrate these checks. (I'm not 
putting these plots in the HTML, but we'll see them in R.)

```{r mixed_diagnostics, eval=FALSE}
out.mcmc <- as.mcmc(out)[[1]]
plot(out.mcmc)
crosscorr.plot(out.mcmc)
mu1 <- c(out.mcmc[ , 1])
sigma <- c(out.mcmc[ , 11])
tau <- c(out.mcmc[ , 12])
theta <- c(out.mcmc[ , 13])
plot(mu1, theta)
plot(sigma, tau)
```

For the sake of time, I'll leave out the use of multiple chains and effective sample sizes, but these are 
very good ideas in general.

# HMM for pollen

In the forward model module, we worked on a HMM with a binomial model
for the data (the likelihood) and a random walk Markov model on the
logit scale as our model for the latent process. Now we're going to
make that model a bit more sophisticated, building on the HMM we saw in the Bayes module.

$$
y_{t}  \sim  \mbox{Binom}(n_{t},\theta_{t}).
$$
$$ 
\mbox{logit}(\theta_t)  \sim  N(\mu + \rho (\mbox{logit}(\theta_{t-1}) - \mu), \tau^2)
$$

$\mu$ allows the model to have an arbitrary overall mean, while $\rho$ controls the auto-correlation over time.
When $\rho = 1$, we have a random walk model, and when $\rho \in (-1, 1)$ we have an autoregressive model. 

Now let's fit the
model to fake data simulated from the model (in this case we know
the statistical model is a good model for the data).

First I'll generate fake data (i.e., simulating from the forward model,
as we did yesterday).

```{r generate_HMM}
nT <- 100
n <- 50
theta0 <- 0.2
tau <- 0.2

logit <- function(x) log(x/(1-x))
expit <- function(x) exp(x) / (1+exp(x))

simHMM <- function(nT, n, mu = 0, rho = 1, tau, theta0) {
  if(length(n) == 1) n <- rep(n, nT)
  theta <- logitTheta <- y <- rep(0, nT)
  for(i in seq_len(nT)) {
    if(i == 1) {
      logitTheta[i] <- rnorm(1, mu + rho*(logit(theta0) - mu), tau)
    } else {
      logitTheta[i] <- rnorm(1, mu + rho*(logitTheta[i-1] - mu), tau)
    }
    y[i] <- rbinom(1, n[i], expit(logitTheta[i]))
  }
  return(data.frame(n = n, y = y, theta = expit(logitTheta)))
}

set.seed(0)
out <- simHMM(n = n, nT = nT, rho = .95, mu = -1, tau = tau, theta0 = theta0)
n <- out$n
y <- out$y
theta <- out$theta
```

Here's the BUGS code for the model. 

```{r hmmFake}
hmmFake <- function(){
	# (hyper)priors
        mu ~ dunif(-5, 5)
        rho ~ dunif(-1, 1)
	tau ~ dunif(0, 100)
	logitTheta0 ~ dnorm(0, .000001)	
    # deterministic transformation
	tau2Inv <- 1/(tau^2)
    
	# latent process evolution and likelihood
	logitTheta[1] ~ dnorm(mu + rho*(logitTheta0 - mu), tau2Inv)
    theta[1] <- exp(logitTheta[1])/(1+exp(logitTheta[1]))  
    y[1] ~ dbin(theta[1], n[1])
	for(i in 2:nT){
		logitTheta[i] ~ dnorm(mu + rho*(logitTheta[i-1] - mu), tau2Inv)
		theta[i] <- exp(logitTheta[i])/(1+exp(logitTheta[i]))
		y[i] ~ dbin(theta[i], n[i])
	}
}
```

Now fit the model using JAGS and look at the fits.

```{r fit_hmmFake, fig.width=7}
out <- jags(data = list(nT = nT, n = n, y = y), parameters.to.save = c('theta', 'mu', 'rho', 'tau'), 
	n.chains = 1, n.iter = 10000, n.burnin = 2000, model.file = hmmFake, DIC = FALSE)
out.mcmc <- as.mcmc(out)[[1]]
colNames <- dimnames(out.mcmc)[[2]]
whichTheta <- grep('theta', colNames)
thetaPost <- out.mcmc[ , whichTheta]
thetaNames <- dimnames(thetaPost)[[2]]
index <- gsub("theta\\[", "", thetaNames)
index <- as.numeric(gsub("\\]", "", index))
thetaPost <- thetaPost[ , order(index)]

plot(seq_len(nT), theta, type = 'l')
lines(seq_len(nT), colMeans(thetaPost), col = 'blue')
points(seq_len(nT), y/n, col = 'red')
plot(seq_len(nT), theta, type = 'l')
quants <- apply(thetaPost, 2, quantile, c(.025, .975))
# lines(seq_len(nT), quants[1, ], col = 'blue', lty = 2)
# lines(seq_len(nT), quants[2, ], col = 'blue', lty = 2)
polygon(cbind(c(seq_len(nT), nT:1, 1), c(quants[1, ], rev(quants[2, ]), quants[1, 1])), border = NA, col = 'lightblue')
lines(seq_len(nT), theta)
lines(seq_len(nT), colMeans(thetaPost), col = 'blue')
points(seq_len(nT), y/n, col = 'red')
par(mfrow = c(1, 3))
nIts <- nrow(out.mcmc)
plot(seq_len(nIts), out.mcmc[ , 1], type = 'l', main = 'mu')
plot(seq_len(nIts), out.mcmc[ , 2], type = 'l', main = 'rho')
plot(seq_len(nIts), out.mcmc[ , 3], type = 'l', main = 'tau')
print(mean(out.mcmc[201:1000, 1]))
print(mean(out.mcmc[201:1000, 2]))
print(mean(out.mcmc[201:1000, 3]))
```

Questions:
- Have we done better than the empirical estimates? Why?
- Any ideas why we might not be able to estimate $\mu$ very well?

Note: in this model, both $\rho$ and $\tau^{2}$ determine how quickly the process
changes over time. Smaller values of $\rho$ and larger values of $\tau$  allow more jumpiness while the reverse force smoother change.

Let's also see how we can choose or generate our own starting values. (See the help on `jags()` for how to set specific initial values.)
I didn't include the output in the HTML, but we can see it in R.

```{r own_start, eval=FALSE}
# with a function
inits <- function(){
	list('mu' = runif(1, -3, 3), 'rho' = runif(1, 0.5, 1), 'tau' = runif(1, .01, 5), 'logitTheta' = rnorm(nT, 0, 3)) 
}
# see the documentation for jags()
out <- jags(data = list(nT = nT, n = n, y = y), inits = inits, parameters.to.save = c('mu', 'tau', 'rho'), 
	n.thin = 1, n.chains = 3, n.iter = 1000, n.burnin = 0, model.file = hmmFake, DIC = FALSE)
traceplot(out, mfrow = c(1, 3), ask = FALSE)
```

Finally let's do an initial fit to real data from Glimmerglass Lake,
which we've already worked with some. First we'll read in the data.


```{r read_data}
data <- read.csv('data/upperMidwest/pollenTimeSeries.csv')
glimmer <- data[data$sitename == "Glimmerglass Lake", ]
glimmer$age <- -glimmer$age
glimmer <- glimmer[order(glimmer$age), ]
glimmer$n <- rowSums(glimmer[ , 12:ncol(glimmer)])
nT <- nrow(glimmer)
plot(glimmer$age, glimmer$hemlock/glimmer$n)
lines(glimmer$age, glimmer$hemlock/glimmer$n)
y <- glimmer$hemlock
n <- round(glimmer$n)
```


Here's the BUGS code for the model. 

```{r hmm}
hmm <- function(){
  mu ~ dunif(-5, 5)
  rho ~ dunif(-1, 1)
  tau ~ dunif(0, 100)
	logitTheta0 ~ dnorm(0, .000001)	
  tau2Inv <- 1/(tau^2)
    
	# latent process evolution and likelihood
	logitTheta[1] ~ dnorm(mu + rho*(logitTheta0 - mu), tau2Inv)
  theta[1] <- exp(logitTheta[1])/(1+exp(logitTheta[1]))  
  y[1] ~ dbin(theta[1], n[1])
	for(i in 2:nT){
		logitTheta[i] ~ dnorm(mu + rho*(logitTheta[i-1] - mu), tau2Inv)
		theta[i] <- exp(logitTheta[i])/(1+exp(logitTheta[i]))
		y[i] ~ dbin(theta[i], n[i])
	}
}
```

And let's fit the model. 

```{r fit_hmm, fig.width=7}
out <- jags(data = list(nT = nT, n = n, y = y), parameters.to.save = c('rho', 'tau', 'theta'), 
	n.chains = 1, n.iter = 10000, n.burnin = 2000, model.file = hmm, DIC = FALSE)
out.mcmc <- as.mcmc(out)[[1]]
thetaHat <- glimmer$hemlock/glimmer$n

colNames <- dimnames(out.mcmc)[[2]]
whichTheta <- grep('theta', colNames)
thetaPost <- out.mcmc[201:1000 , whichTheta]
thetaNames <- dimnames(thetaPost)[[2]]
index <- gsub("theta\\[", "", thetaNames)
index <- as.numeric(gsub("\\]", "", index))
thetaPost <- thetaPost[ , order(index)]

par(mfrow = c(1,1))
plot(glimmer$age, thetaHat, col = 'red')
quants <- apply(thetaPost, 2, quantile, c(.025, .975))
polygon(cbind(c(glimmer$age, rev(glimmer$age), glimmer$age[1]), c(quants[1, ], 
	rev(quants[2, ]), quants[1, 1])), border = NA, col = 'lightblue')
lines(glimmer$age, thetaHat, col = 'red')
points(glimmer$age, thetaHat, col = 'red')
lines(glimmer$age, colMeans(thetaPost), col = 'blue')
title('hemlock at Glimmerglass Lake')
abline(v = 1650 - 1950, col = 'grey')
```

We'll check the mixing in R and see that this chain has not run for nearly long enough.

# Next steps

- Discussion questions
  + How we would code up a model that treats depth
or radiocarbon years as the time index and correctly accounts for
the irregular spacing of data.
  + Let's consider some of the aspects of the data
and the scientific context that have been left out of our model for
pollen. 
- Practice problems   
  + Change the prior on $\tau$ (currently uniform, as recommended in Gelman (2006; Bayesian Analysis 1:515)) to be an gamma prior on $\tau^2$ with parameters 0.001 and 0.001. Assess whether the inference changes for either $\tau$ or the species composition.
  + (Advanced) Rework the JAGS code for Glimmerglass Lake to use radiocarbon years as the time index.
