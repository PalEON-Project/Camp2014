#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{/accounts/gen/vis/paciorek/latex/paciorek-asa,times,graphics}
%\input{~paciorek/latex/paciorekMacros}
%\renewcommand{\baselinestretch}{1.5}
\end_preamble
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Camp PalEON Bayesian Statistics Module
\end_layout

\begin_layout Standard
In this module, we'll cover the basics of Bayesian statistics, hierarchical
 modeling, and MCMC.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[perhaps focus on AR1 not RW model so students more likely to get smoother
 sample paths]
\end_layout

\end_inset


\end_layout

\begin_layout Section
The Bayesian recipe: Likelihood, prior and posterior
\end_layout

\begin_layout Itemize
Prior: 
\begin_inset Formula $g(\theta)$
\end_inset

 (a pdf or pmf) summarizes our beliefs about 
\begin_inset Formula $\theta$
\end_inset

 before we collect (or analyze) the dataset we have.
 
\begin_inset Formula $\theta$
\end_inset

 might be a single thing or a vector of things.
\end_layout

\begin_layout Itemize
Posterior: 
\begin_inset Formula $g(\theta|Y_{1}=y_{1},\ldots,Y_{n}=y_{n})$
\end_inset

 (a pdf or pmf) summarizes our new beliefs informed by the dataset.
\end_layout

\begin_deeper
\begin_layout Itemize
Generically, 
\begin_inset Formula $g(\theta|y)$
\end_inset

 where 
\begin_inset Formula $y=\{y_{1},\ldots,y_{n}\}$
\end_inset

 indicates the vector of data values.
\end_layout

\begin_layout Itemize
Note that this is a conditional distribution, given the data, which is intuitive.
\end_layout

\begin_layout Itemize
Our goal is to find the posterior mathematically and interpret it scientifically.
\end_layout

\end_deeper
\begin_layout Itemize
We use Bayes theorem to find 
\begin_inset Formula $g(\theta|y)$
\end_inset

, making use of 
\begin_inset Formula $f(y|\theta)$
\end_inset

, by using a version of Bayes theorem for random variables rather than events:
\begin_inset Formula 
\[
g(\theta|Y=y)=\frac{f(y|\theta)g(\theta)}{g(y)}\propto f(y|\theta)g(\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
We use the likelihood, 
\begin_inset Formula $f(y|\theta)$
\end_inset

 (pdf or pmf of the data) to update the prior.
\end_layout

\begin_layout Section
A basic example with binomial data
\end_layout

\begin_layout Standard
We'll illustrate a simple Bayesian analysis using composition data for a
 single species from a single site.
 Suppose we are particularly interested in oak composition and we count
 pollen grains and record the number of oak and number of non-oak grains.
 We want to know what proportion, 
\begin_inset Formula $\theta$
\end_inset

, of oak grains there are in the entire 'population', not just the 
\begin_inset Formula $n=100$
\end_inset

 grains that we counted.
\end_layout

\begin_layout Standard
What is the likelihood? I.e., what is the distribution of the data we observe
 given the unknown of interest?
\end_layout

\begin_layout Standard
Ok, now the prior.
 
\series bold
Question
\series default
: What could I use as a distribution for the unknown that reflects not having
 any beliefs in advance of seeing the data? First think about what the possible
 values of 
\begin_inset Formula $\theta$
\end_inset

 are.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
g(\theta|y) & \propto & f(y|\theta)g(\theta)\\
 & = & {n \choose y}\theta^{y}(1-\theta)^{n-y}\cdot1\\
 & \propto & \theta^{y}(1-\theta)^{n-y}
\end{eqnarray*}

\end_inset

So in some sense we are done.
 We can plot this expression AS A FUNCTION OF 
\begin_inset Formula $\theta$
\end_inset

, not of 
\begin_inset Formula $y$
\end_inset

.
 
\end_layout

\begin_layout Standard
Often we want to know the posterior distribution for 
\begin_inset Formula $\theta$
\end_inset

 or the expected value of the posterior distribution, or the variance/standard
 deviation.
 
\end_layout

\begin_layout Subsection
The beta distribution
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\theta$
\end_inset

 is a continuous random variable whose values must be in 
\begin_inset Formula $(0,1)$
\end_inset

.
 I.e., 
\begin_inset Formula $\theta$
\end_inset

 could represent a probability.
\end_layout

\begin_layout Standard
A distribution that is often used to represent uncertainty in such a random
 variable is the beta distribution, which has parameters 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 If a random variable 
\begin_inset Formula $\theta$
\end_inset

 is distributed according to a beta distribution, its pdf is:
\begin_inset Formula 
\[
f(\theta)=\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $B(\alpha,\beta)$
\end_inset

 is a mathematical function that cannot be evaluated in closed form but
 can be calculated on a computer.
 Confusingly, it's called the 'beta' function.
 
\end_layout

\begin_layout Standard
The mean and variance of the distribution are: 
\begin_inset Formula $E(\theta)=\frac{\alpha}{\alpha+\beta}$
\end_inset

; 
\begin_inset Formula $Var(\theta)=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$
\end_inset

.
\end_layout

\begin_layout Standard
As we change 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, we get different beta distributions.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename betaDist.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Some different beta distributions as you change 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Revisiting our example
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
g(\theta|y)\propto\theta^{y}(1-\theta)^{n-y}
\]

\end_inset


\end_layout

\begin_layout Standard
What is the distribution of 
\begin_inset Formula $\theta$
\end_inset

?
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $y=35$
\end_inset

 and 
\begin_inset Formula $n=100$
\end_inset

.
\end_layout

\begin_layout Chunk
<<flatprior, out.width=4in>>=
\end_layout

\begin_layout Chunk
y <- 35; n <- 100
\end_layout

\begin_layout Chunk
thetaSeq <- seq(0, 1, length = 200)
\end_layout

\begin_layout Chunk
lik <- dbinom(y, n, thetaSeq)
\end_layout

\begin_layout Chunk
prior <- rep(1, 200)
\end_layout

\begin_layout Chunk
alphaPost <- y+1; betaPost <- n-y+1
\end_layout

\begin_layout Chunk
post <- dbeta(thetaSeq, alphaPost, betaPost)
\end_layout

\begin_layout Chunk
plot(thetaSeq, post, type = 'l')
\end_layout

\begin_layout Chunk
lines(thetaSeq, prior, col = 'red')
\end_layout

\begin_layout Chunk
lines(thetaSeq, lik*80, col = 'blue') # scaling of likelihood is arbitrary
 -- it's not a density for theta
\end_layout

\begin_layout Chunk
legend(0, 8, legend = c('prior', 'lik', 'post'), col = c('red','blue','black'),
 lty = rep(1,3), bty = 'n')
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
That illustrated prior ignorance.
 Now suppose we think we know something about this location, and we have
 some idea about the proportion of oak in the forest.
 (Perhaps based on a previous study.)
\end_layout

\begin_layout Standard
For the moment (we'll see why in a minute), let's use a beta distribution
 to capture our belief about 
\begin_inset Formula $p$
\end_inset

 in advance of seeing the data.
 Suppose we think there is about 20% oak in the forest with a standard deviation
 of 10%.
 We can roughly capture this belief using a beta distribution with 
\begin_inset Formula $\alpha=3$
\end_inset

, 
\begin_inset Formula $\beta=12$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Challenge
\series default
: How did I come up with 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

?
\end_layout

\begin_layout Standard
Let's go back to likelihood times prior:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
g(\theta|y) & \propto & f(y|\theta)g(\theta)\\
 & = & {n \choose y}\theta^{y}(1-\theta)^{n-y}\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
 & \propto & \theta^{y}(1-\theta)^{n-y}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
 & = & \theta^{y+\alpha-1}(1-\theta)^{n-y+\beta-1}
\end{eqnarray*}

\end_inset


\series bold
Questions
\series default
: What is the posterior distribution of 
\begin_inset Formula $p$
\end_inset

 now? What is the posterior expected value? What is the posterior variance?
\end_layout

\begin_layout Chunk
<<infPrior, out.width=4in>>=
\end_layout

\begin_layout Chunk
y <- 35; n <- 100
\end_layout

\begin_layout Chunk
thetaSeq <- seq(0, 1, length = 200)
\end_layout

\begin_layout Chunk
lik <- dbinom(y, n, thetaSeq)
\end_layout

\begin_layout Chunk
alphaPrior <- 3; betaPrior <- 12
\end_layout

\begin_layout Chunk
prior <- dbeta(thetaSeq, alphaPrior, betaPrior) 
\end_layout

\begin_layout Chunk
alphaPost <- y+alphaPrior; betaPost <- n-y+betaPrior
\end_layout

\begin_layout Chunk
post <- dbeta(thetaSeq, alphaPost, betaPost)
\end_layout

\begin_layout Chunk
plot(thetaSeq, post, type = 'l')
\end_layout

\begin_layout Chunk
lines(thetaSeq, prior, col = 'red')
\end_layout

\begin_layout Chunk
lines(thetaSeq, lik*80, col = 'blue')  # scaling arbitrary
\end_layout

\begin_layout Chunk
legend(0, 8, legend = c('prior', 'lik', 'post'), col = c('red','blue','black'),
 lty = rep(1,3), bty = 'n')
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Let's see some different cases where we vary the sample size and see how
 influential the prior distribution is.
\end_layout

\begin_layout Subsection
Using the posterior distribution 
\end_layout

\begin_layout Standard
If we know the posterior as an actual distribution, we can use the posterior
 mean (or median) as our estimate and get a 95% uncertainty (credible) interval
 from the 2.5th and 97.5th percentiles of the distribution.
\end_layout

\begin_layout Chunk
<<posteriorInference, out.width=4in>>=
\end_layout

\begin_layout Chunk
y <- 35; n <- 100
\end_layout

\begin_layout Chunk
alphaPost <- y+1; betaPost <- n-y+1
\end_layout

\begin_layout Chunk
qbeta(c(.025, .5, .975), alphaPost, betaPost)
\end_layout

\begin_layout Chunk
postMean <- alphaPost/(alphaPost + betaPost)
\end_layout

\begin_layout Chunk
postSD <- sqrt(alphaPost*betaPost / ((alphaPost + betaPost)^2 * (alphaPost
 + betaPost + 1)))
\end_layout

\begin_layout Chunk
postMean
\end_layout

\begin_layout Chunk
postSD
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Often we won't know the distribution as a standard distribution but we will
 have a sample from the distribution.
 In particular we'll often have output from an MCMC.
 Recall from the statistics module that we can use empirical quantities
 calculated from a sample from a distribution to approximate the characteristics
 of the distribution.
\end_layout

\begin_layout Chunk
<<posteriorInferenceWithSample, out.width=4in>>=
\end_layout

\begin_layout Chunk
####### PAY NO ATTENTION TO THE MAN BEHIND THE CURTAIN ######
\end_layout

\begin_layout Chunk
fakeMCMCsample <- rbeta(2000, alphaPost, betaPost) 
\end_layout

\begin_layout Chunk
#############################################################
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
# now use 'fakeMCMCsample' as if it came from an actual MCMC, 
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
# ignoring the fact that theta has a beta distribution
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
quantile(fakeMCMCsample, c(.025, .5, .975))
\end_layout

\begin_layout Chunk
mean(fakeMCMCsample)
\end_layout

\begin_layout Chunk
sd(fakeMCMCsample)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Section
Hierarchical models and latent processes
\end_layout

\begin_layout Standard
The basic idea of a hierarchical model is to create a mathematical representatio
n (a model) using the language of probability distributions to capture the
 main features of our data and the underlying process of interest.
 Our goal is generally to either better understand features of the underlying
 process or make predictions based on the model.
\end_layout

\begin_layout Subsection
A basic hierarchical model
\end_layout

\begin_layout Standard
The most basic hierarchical model has a data layer, a single latent layer,
 and a layer of (hyper)parameters.
 Let's suppose we have data on rats in a laboratory (say some metabolite
 in blood), with each mother rat having multiple pups, and with us measuring
 some outcome on each pup.
 We want a model that accounts for the fact that pups within a litter are
 related.
 Option A would be to try to introduce some sort of correlation structure
 into how we model the data.
 But let's first consider an alternative, option B, that adds some variables
 to help guide our thinking about the process at hand.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
y_{im} & \sim & \mathcal{N}(\mu_{m},\sigma^{2});\, i=1,\ldots,n_{m}\\
\mu_{m} & \sim & \mathcal{N}(\theta,\tau^{2})\, m=1,\ldots,M
\end{eqnarray*}

\end_inset

On the board, we'll consider a graphical representation of this model.
 (In the BUGS manuals and examples, you'll see lots of model structures
 illustrated as graphs.)
\end_layout

\begin_layout Standard
Now let's return to Option A.
 In a model with latent values such as this one, a basic statistical approach
 to dealing with the high dimensionality is to integrate over the latent
 values.
 If we do this (integrating over 
\begin_inset Formula $\mu=(\mu_{1},\ldots,\mu_{M})$
\end_inset

, we end up with the following model, with a so-called compound symmetry
 correlation structure.
\end_layout

\begin_layout Standard
So far there hasn't really been a scientific question; we've just tried
 to model the structure of the data.
 Let's take the analogue of a simple t-test.
 Assume we have two groups of mothers and we test a drug or a chemical in
 one group and use the other group as the placebo.
 
\end_layout

\begin_layout Standard

\series bold
Challenge
\series default
: How would we change our initial model and what quantity in the model is
 now the main focus of interest? What would a graph of the model look like?
\end_layout

\begin_layout Standard
Now let's talk through what would be needed to modify this model to relate
 to vegetation data.
 Let's consider stands of trees within a forest.
 Question: What aspects of the model would change?
\end_layout

\begin_layout Standard
Note: henceforth I'll refer to all the unknowns (latent process values and
 hyperparameters) as the 'parameters'.
 Basically anything that is not data or fixed constants (such as sample
 sizes and covariates/predictors) is a parameter in the model.
\end_layout

\begin_layout Subsection
Prediction
\end_layout

\begin_layout Standard
Let's think about how we would make predictions from this model.
 Suppose I either know all the unknown parameters and latent values or have
 estimates for them.
\end_layout

\begin_layout Standard

\series bold
Questions
\series default
:
\end_layout

\begin_layout Enumerate
What is our best guess for the 
\begin_inset Formula $y$
\end_inset

 for a new pup from the first mother?
\end_layout

\begin_layout Enumerate
What is our best guess for the 
\begin_inset Formula $\mu$
\end_inset

 for a mother for whom we have no data?
\end_layout

\begin_layout Enumerate
What is our best guess for the 
\begin_inset Formula $y$
\end_inset

 for a new pup from a mother for which we don't have data?
\end_layout

\begin_layout Enumerate
Comparing #1 and #3, for which one should our uncertainty be greater? Let's
 consider 
\begin_inset Formula $\mbox{Var}(y|\mu)$
\end_inset

 and 
\begin_inset Formula $\mbox{Var}(y|\theta)$
\end_inset

, but first let's consider questions 5-7.
\end_layout

\begin_layout Enumerate
How can I draw a sample of possible 
\begin_inset Formula $y$
\end_inset

's for a new pup from the first mother?
\end_layout

\begin_layout Enumerate
How can I draw a sample of possible 
\begin_inset Formula $y$
\end_inset

's for a new pup from a mother with no data?
\end_layout

\begin_layout Enumerate
How does our construction of new 
\begin_inset Formula $y$
\end_inset

's support our conclusion in #4?
\end_layout

\begin_layout Standard
The distribution of new data is called the predictive distribution.
 
\end_layout

\begin_layout Subsection
Adding complexity to the latent layer
\end_layout

\begin_layout Standard
Let's consider a model for the evolution of sediment pollen over time.
 Unlike the above example, we want the latent values to be related to each
 other in a manner informed by the time structure.
 
\end_layout

\begin_layout Standard
One common model in these contexts, if we treat time as being discrete,
 is a hidden Markov model (HMM), also known as a dynamic linear model (DLM)
 and as a state space model.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & \sim & \mathcal{N}(\theta_{t},\sigma^{2})\\
\theta_{t} & \sim & \mathcal{N}(\theta_{t-1},\tau^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The first equation is the likelihood, or the noise model.
 The second equation is the model for the latent (or hidden) process, which
 may be called the state model or the latent process.
 It's a 'hidden' model because we don't directly observe the process.
 It's a Markov model because the latent process value at time 
\begin_inset Formula $t$
\end_inset

 depends only on the process value at the previous time step.
 
\end_layout

\begin_layout Standard
To complete the Bayesian model, we need prior distributions for the parameters
 and for the initial state, 
\begin_inset Formula $g(\sigma^{2},\tau^{2},\theta_{0})$
\end_inset

.
\end_layout

\begin_layout Standard
For the statistics module, you wrote code in R to simulate from a variation
 on this model.
 That 'forward' simulation took the parameters as known and simulated data
 that might be generated from such a model.
 Our Bayesian analysis in the simple binomial model for oak pollen above
 did the reverse: it took the data and a model structure and tried to reconstruc
t what the plausible values of the unknown parameters and latent values
 are.
 Thus our Bayesian statistical analysis is also known as an 'inverse' problem.
\end_layout

\begin_layout Subsection
General DLMs
\end_layout

\begin_layout Standard
More generally, we might have data and process values that relate to each
 other in more complicated ways.
 For example, we might have the following where 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $\theta_{t}$
\end_inset

 are vectors at each time step:
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & \sim & \mathcal{N}(H_{t}\theta_{t},R_{t})\\
\theta_{t} & \sim & \mathcal{N}(F_{t}\theta_{t-1},\Sigma_{t})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This set of equations relates to an algorithm known as the Kalman filter,
 which is a way to estimate 
\begin_inset Formula $\theta_{t}$
\end_inset

 given data 
\begin_inset Formula $y_{t}$
\end_inset

 and an estimate of the previous state, 
\begin_inset Formula $\hat{\theta}_{t-1}$
\end_inset

.
 One way to think about the Kalman filter is that it is just as a hierarchical
 model where we use probability manipulations to estimate 
\begin_inset Formula $\theta_{t}$
\end_inset

 over time and to make predictions into the future.
\end_layout

\begin_layout Standard
And of course the distributions might not be normal.
 Here one gets into more advanced variants on the Kalman filter.
\end_layout

\begin_layout Section
Our first MCMC
\end_layout

\begin_layout Standard
Markov chain Monte Carlo is a computational technique used to simulate a
 sample from a posterior distribution.
 There are a variety of MCMC approaches.
 
\end_layout

\begin_layout Standard
One basic strategy is called Gibbs sampling - it involves cycling through
 the parameters and sampling from
\begin_inset Formula 
\begin{eqnarray*}
g(\theta_{1}|\theta_{2},\ldots,\theta_{p},y)\\
g(\theta_{2}|\theta_{1},\theta_{3},\ldots,\theta_{p},y)\\
\cdots\\
g(\theta_{p}|\theta_{1},\ldots,\theta_{p-1},y)
\end{eqnarray*}

\end_inset

In this case, 
\begin_inset Formula $\theta$
\end_inset

 would include ALL of the unknown parameters, both latent process values
 and hyperparameters.
\end_layout

\begin_layout Standard
Let's see an example of this.
 We'll use a bivariate normal distribution to illustrate things.
 Let's assume that we 'forget' that we know how to simulate directly from
 a bivariate normal.
 
\end_layout

\begin_layout Standard
Suppose we've done the math to figure out that 
\begin_inset Formula 
\begin{eqnarray*}
\theta_{1}|\theta_{2},y & \sim & \mbox{\mathcal{N}}\left(\mu_{1}+\sigma_{1}\rho\frac{\theta_{2}-\mu_{2}}{\sigma_{2}},\sigma_{1}^{2}(1-\rho^{2})\right)\\
\theta_{2}|\theta_{1},y & \sim & \mbox{\mathcal{N}}\left(\mu_{2}+\sigma_{2}\rho\frac{\theta_{1}-\mu_{1}}{\sigma_{1}},\sigma_{2}^{2}(1-\rho^{2})\right)
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\mu_{1}=0$
\end_inset

, 
\begin_inset Formula $\mu_{2}=1$
\end_inset

, 
\begin_inset Formula $\sigma_{1}=\sigma_{2}=1$
\end_inset

, 
\begin_inset Formula $\rho=0.7$
\end_inset

 are functions of 
\begin_inset Formula $y$
\end_inset

.
 This example is particularly useful because in many models, the posterior
 distribution of 
\begin_inset Formula $\theta$
\end_inset

 is approximately multivariate normal distributed.
 
\end_layout

\begin_layout Standard
So let's set up a Gibbs sampler to draw samples from 
\begin_inset Formula $g(\theta_{1},\theta_{2}|y)$
\end_inset


\end_layout

\begin_layout Chunk
<<bivarNorm, out.width=4in>>=
\end_layout

\begin_layout Chunk
nIts <- 1000
\end_layout

\begin_layout Chunk
nPlot <- 40
\end_layout

\begin_layout Chunk
mu1 <- 0; mu2 <- 1; sig1 <- 1; sig2 <- 1; rho <- .7 
\end_layout

\begin_layout Chunk
store <- matrix(NA, nrow = nIts, ncol = 2)
\end_layout

\begin_layout Chunk
luckyStartingValues <- c(.1, 1.2)
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
store[1, ] <- luckyStartingValues
\end_layout

\begin_layout Chunk
for(it in 2:nIts){
\end_layout

\begin_layout Chunk
	curTheta2 <- store[it-1, 2]
\end_layout

\begin_layout Chunk
	curTheta1 <- rnorm(1, mu1 + sig1*rho*(curTheta2 - mu2)/sig2, sig1*sqrt(1-rho^2)
)
\end_layout

\begin_layout Chunk
	curTheta2 <- rnorm(1, mu2 + sig2*rho*(curTheta1 - mu1)/sig1, sig2*sqrt(1-rho^2)
)
\end_layout

\begin_layout Chunk
	store[it, ] <- c(curTheta1, curTheta2)
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk
# we know the 'true' 'posterior', so let's plot its contours to see how
 well we are doing
\end_layout

\begin_layout Chunk
library(mvtnorm)
\end_layout

\begin_layout Chunk
gridLength <- 40
\end_layout

\begin_layout Chunk
grid1d <- seq(-8, 3, length = gridLength)
\end_layout

\begin_layout Chunk
grid2d <- expand.grid(grid1d, grid1d)
\end_layout

\begin_layout Chunk
densValues <- dmvnorm(grid2d, c(mu1, mu2), matrix(c(sig1^2, rho*sig1*sig2,
 rho*sig1*sig2, sig2^2), 2, 2))
\end_layout

\begin_layout Chunk
contour(grid1d, grid1d, matrix(densValues, gridLength))
\end_layout

\begin_layout Chunk
lines(store[1:nPlot, 1], store[1:nPlot, 2], col = 'blue')
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
store2 <- matrix(NA, nrow = nIts, ncol = 2)
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
unluckyStartingValues <- c(-6, -8)
\end_layout

\begin_layout Chunk
store2[1, ] <- unluckyStartingValues
\end_layout

\begin_layout Chunk
for(it in 2:nIts){
\end_layout

\begin_layout Chunk
	curTheta2 <- store2[it-1, 2]
\end_layout

\begin_layout Chunk
	curTheta1 <- rnorm(1, mu1 + sig1*rho*(curTheta2 - mu2)/sig2, sig1*sqrt(1-rho^2)
)
\end_layout

\begin_layout Chunk
	curTheta2 <- rnorm(1, mu2 + sig2*rho*(curTheta1 - mu1)/sig1, sig2*sqrt(1-rho^2)
)
\end_layout

\begin_layout Chunk
	store2[it, ] <- c(curTheta1, curTheta2)
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk
lines(store2[1:nPlot, 1], store2[1:nPlot, 2], col = 'red')
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Often when we want to use this sequential updating strategy, the conditional
 distribution of a parameter is not a known distribution from which we can
 easily sample.
 In such cases, we often use Metropolis (or Metropolis-Hastings) sampling.
 Other techniques such as adaptive rejection sampling are also used.
 We won't go into these, but they are used 'under the hood' in BUGS and
 other software.
\end_layout

\begin_layout Subsection
Convergence and mixing of MCMC chains
\end_layout

\begin_layout Standard
MCMC chains can suffer from two major problems:
\end_layout

\begin_layout Enumerate
The chain takes a long time to converge (bad starting values).
\end_layout

\begin_layout Enumerate
The chain has converged but is mixing slowly (the samples are highly dependent).
\end_layout

\begin_layout Standard
We can take a look at these issues using 'trace plots' of the MCMC output.
\end_layout

\begin_layout Chunk
<<convergence, out.width=4in>>=
\end_layout

\begin_layout Chunk
par(mfrow = c(2,2))
\end_layout

\begin_layout Chunk
subset <- 1:100
\end_layout

\begin_layout Chunk
tsplot <- function(vec, ...){
\end_layout

\begin_layout Chunk
	plot(1:length(vec), vec, xlab = 'index', ylab = '', ...)
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk
tsplot(store[subset, 1], main = 'theta1, good start', type = 'l')
\end_layout

\begin_layout Chunk
tsplot(store[subset, 2], main = 'theta2, good start', type = 'l')
\end_layout

\begin_layout Chunk
tsplot(store2[subset, 1], main = 'theta1, bad start', type = 'l')
\end_layout

\begin_layout Chunk
tsplot(store2[subset, 2], main = 'theta2, bad start', type = 'l')
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
To deal with the (1) the convergence issues, we throw out the initial part
 of the chain and don't use it to make our estimates.
 In this case we only have to throw out a few values even with the bad starting
 values, but in many situations, we need to throw out thousands of samples.
 (2) Mixing is a problem because we have fewer effectively-independent samples.
 Recall our discussion in the statistics module that if we have a large
 sample from a distribution, that sample is a good representation of the
 distribution.
 If we have a small sample, it may not be.
 When we have highly-correlated samples we don't have as good a representation
 as when the samples are independent.
\end_layout

\begin_layout Standard
Two of the main causes of convergence and mixing problems are bad starting
 values and high correlation between parameters.
 There are strategies for 'blocking' parameters that try to allow the parameters
 to explore the parameter space in a way that respects the correlation.
 I'll illustrate this on the board.
\end_layout

\begin_layout Subsection
Using the output of MCMC chains
\end_layout

\begin_layout Standard
The basic story here is that the chain output is a sample from the posterior
 (provided we've thrown away values from the burn-in period).
 We can use the sample to approximate the posterior mean, posterior sd,
 and posterior quantiles.
\end_layout

\begin_layout Chunk
<<usingPosterior>>=
\end_layout

\begin_layout Chunk
library(coda) # a package for manipulating MCMC output
\end_layout

\begin_layout Chunk
nonBurn <- 100:nIts  # conservative in this case
\end_layout

\begin_layout Chunk
use <- store[nonBurn, ]
\end_layout

\begin_layout Chunk
ess1 <- effectiveSize(use[, 1])
\end_layout

\begin_layout Chunk
ess2 <- effectiveSize(use[, 2])
\end_layout

\begin_layout Chunk
postMean <- colMeans(use)
\end_layout

\begin_layout Chunk
postSD <- apply(use, 2, sd)
\end_layout

\begin_layout Chunk
uncIntervals <- apply(use, 2, quantile, c(.025, .975))
\end_layout

\begin_layout Chunk
postCorr <- cor(use[, 1], use[ , 2])
\end_layout

\begin_layout Chunk
postMean; postSD; uncIntervals; postCorr
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Apart from our posterior correlation estimate, the other quantities we calculate
d represent the marginal posteriors: 
\begin_inset Formula $P(\theta_{1}|y)$
\end_inset

 and 
\begin_inset Formula $g(\theta_{2}|y)$
\end_inset

.
 In this context we can just ignore the samples of the parameters we are
 not interested in.
 We're generally not interested in conditional posteriors such as 
\begin_inset Formula $g(\theta_{1}|\theta_{2},y)$
\end_inset

, but if we were, we could have just fixed 
\begin_inset Formula $\theta_{2}$
\end_inset

 at the value we want to condition on.
\end_layout

\begin_layout Standard
MCMC output provides a huge amount of flexibility.
 If I am interested in some function of 
\begin_inset Formula $\theta$
\end_inset

, I can just find the function of each of the samples, and that gives me
 a sample of the function.
 Suppose 
\begin_inset Formula $\theta_{1}$
\end_inset

 was on the log scale and I want it on the original scale.
\end_layout

\begin_layout Chunk
<<posteriorFunctionals>>=
\end_layout

\begin_layout Chunk
derivedQuant <- exp(use[ , 1])
\end_layout

\begin_layout Chunk
postMeanDer <- mean(derivedQuant)
\end_layout

\begin_layout Chunk
postCIsDer <- quantile(derivedQuant, c(.025, .975))
\end_layout

\begin_layout Chunk
postMeanDer; postCIsDer
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Section
JAGS/BUGS/WinBUGS
\end_layout

\begin_layout Standard
Let's work through our mixed model example and the DLM example using JAGS
 (or BUGS).
\end_layout

\begin_layout Subsection
Mixed model
\end_layout

\begin_layout Standard
Here we'll use simulated data.
\end_layout

\begin_layout Chunk
<<mixedModelData, eval=FALSE>>=
\end_layout

\begin_layout Chunk
theta <- 2
\end_layout

\begin_layout Chunk
sig <- 2
\end_layout

\begin_layout Chunk
nMoms <- 10
\end_layout

\begin_layout Chunk
#### IGNORE THE MAN BEHIND THE CURTAIN ####
\end_layout

\begin_layout Chunk
mus <- rgamma(nMoms, shape = 2, scale = 1)
\end_layout

\begin_layout Chunk
nPups <- rbinom(nMoms, 15, prob = .4) 
\end_layout

\begin_layout Chunk
momIds <- rep(1:nMoms, times = nPups)
\end_layout

\begin_layout Chunk
y <- rnorm(sum(nPups), mus[momIds], sig)
\end_layout

\begin_layout Chunk
n <- sum(nPups)
\end_layout

\begin_layout Chunk
###########################################
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Here's the BUGS code for the model.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

mixedModel <- function(){
\end_layout

\begin_layout Plain Layout

  # (hyper)priors
\end_layout

\begin_layout Plain Layout

  sig ~ dunif(0, 100)
\end_layout

\begin_layout Plain Layout

  tau ~ dunif(0, 100)
\end_layout

\begin_layout Plain Layout

  theta ~ dnorm(0,.00001)
\end_layout

\begin_layout Plain Layout

  # some transformations to get precisions from sd's
\end_layout

\begin_layout Plain Layout

  tau2Inv <- 1/(tau^2)
\end_layout

\begin_layout Plain Layout

  sig2Inv <- 1/(sig^2)
\end_layout

\begin_layout Plain Layout

  # latent value layer
\end_layout

\begin_layout Plain Layout

  for(i in 1:nMoms){
\end_layout

\begin_layout Plain Layout

    mus[i] ~ dnorm(theta, tau2Inv)
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  # likelihood
\end_layout

\begin_layout Plain Layout

  for(j in 1:n)
\end_layout

\begin_layout Plain Layout

    {
\end_layout

\begin_layout Plain Layout

      y[j] ~ dnorm(mus[momIds[j]], sig2Inv)
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here's how we run it from R.
\end_layout

\begin_layout Chunk
<<mixedModelRun, eval=FALSE>>=
\end_layout

\begin_layout Chunk
library(R2jags) # or BRugs or R2WinBUGS or R2OpenBUGS
\end_layout

\begin_layout Chunk
out <- jags(data = list(y = y, n = n, nMoms = nMoms, momIds = momIds),
\end_layout

\begin_layout Chunk
  parameters.to.save = c('theta','sig','tau', 'mus'), n.chains = 1,
\end_layout

\begin_layout Chunk
  n.iter = 2000, n.burnin = 1000, model.file = mixedModel, DIC = FALSE)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Standard
A few things to check to see if the MCMC is performing well:
\end_layout

\begin_layout Enumerate
Look at the trace plots and see if the initial part of the chain looks similar
 to the later part.
 If not, you need a longer burn-in period.
\end_layout

\begin_layout Enumerate
Look at the trace plots and see if the chain is mixing.
 We want to see rapid movement up and down, not slow excursions.
\end_layout

\begin_layout Enumerate
See if there are any posterior correlations between parameters that are
 near 1 or -1, which indicate trading off between parameters.
\end_layout

\begin_layout Enumerate
Run the model with multiple chains (with different starting values) and
 see that all the chains converge to the same posterior distribution.
\end_layout

\begin_layout Standard
Let's look at some figures that illustrate these checks.
\end_layout

\begin_layout Chunk
<<mixedModelPostProcess, eval=FALSE>>=
\end_layout

\begin_layout Chunk
out.mcmc <- as.mcmc(out)
\end_layout

\begin_layout Chunk
plot(out)
\end_layout

\begin_layout Chunk
crosscorr.plot(out.mcmc)
\end_layout

\begin_layout Chunk
mu1 <- c(out.mcmc[ , 2])
\end_layout

\begin_layout Chunk
sig <- c(out.mcmc[ , 12])
\end_layout

\begin_layout Chunk
tau <- c(out.mcmc[ , 13])
\end_layout

\begin_layout Chunk
theta <- c(out.mcmc[ , 14])
\end_layout

\begin_layout Chunk
plot(mu1, theta)
\end_layout

\begin_layout Chunk
plot(sig, tau)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Subsection
HMM for pollen
\end_layout

\begin_layout Standard
In the statistics module, we worked on a HMM with a binomial model for the
 data (the likelihood) and a random walk Markov model on the logit scale
 as our model for the latent process.
 First let's fit the model to fake data simulated from the model (in this
 case we know the statistical model is a good model for the data).
\end_layout

\begin_layout Chunk
<<fakeDLMdata, eval=FALSE>>=
\end_layout

\begin_layout Chunk
nT <- 100
\end_layout

\begin_layout Chunk
n <- 100
\end_layout

\begin_layout Chunk
### IGNORE THE MAN BEHIND THE CURTAIN #######
\end_layout

\begin_layout Chunk
theta0 <- 0.2
\end_layout

\begin_layout Chunk
tau <- 0.2
\end_layout

\begin_layout Chunk
tau2Inv <- 1/(tau^2)
\end_layout

\begin_layout Chunk
logitTheta <- p <- y <- rep(NA, nT)
\end_layout

\begin_layout Chunk
logitTheta0 <- log(theta0/(1-theta0))
\end_layout

\begin_layout Chunk
logitTheta[1] <- rnorm(1, logitTheta0, tau)
\end_layout

\begin_layout Chunk
theta[1] <- exp(logitTheta[1])/(1+exp(logitTheta[1]))
\end_layout

\begin_layout Chunk
y[1] <- rbinom(1, n, theta[1])
\end_layout

\begin_layout Chunk
for(i in 2:nT){
\end_layout

\begin_layout Chunk
	logitTheta[i] <- rnorm(1, logitTheta[i-1], tau)
\end_layout

\begin_layout Chunk
	theta[i] <- exp(logitTheta[i])/(1+exp(logitTheta[i]))
\end_layout

\begin_layout Chunk
	y[i] <- rbinom(1, n, theta[i])
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk
##############################################
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Here's the BUGS code for the model.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

dlmFake <- function(){
\end_layout

\begin_layout Plain Layout

	# (hyper)priors
\end_layout

\begin_layout Plain Layout

	tau ~ dunif(0, 100)
\end_layout

\begin_layout Plain Layout

	logitTheta0 ~ dnorm(0, .000001)	
\end_layout

\begin_layout Plain Layout

    # deterministic transformation
\end_layout

\begin_layout Plain Layout

	tau2Inv <- 1/(tau^2)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

	# latent process evolution and likelihood
\end_layout

\begin_layout Plain Layout

	logitTheta[1] ~ dnorm(logitTheta0, tau2Inv)
\end_layout

\begin_layout Plain Layout

    theta[1] <- exp(logitTheta[1])/(1+exp(logitTheta[1]))
\end_layout

\begin_layout Plain Layout

    y[1] ~ dbin(theta[1], n)
\end_layout

\begin_layout Plain Layout

	for(i in 2:nT){
\end_layout

\begin_layout Plain Layout

		logitTheta[i] ~ dnorm(logitTheta[i-1], tau2Inv)
\end_layout

\begin_layout Plain Layout

		theta[i] <- exp(logitTheta[i])/(1+exp(logitTheta[i]))
\end_layout

\begin_layout Plain Layout

		y[i] ~ dbin(theta[i], n)
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Standard
Now fit the model using JAGS and look at the fits.
\end_layout

\begin_layout Chunk
<<fakeDLMfit, eval=FALSE>>=
\end_layout

\begin_layout Chunk
out <- jags(data = list(nT = nT, n = n, y = y), parameters.to.save = c('tau',
 'theta'), 
\end_layout

\begin_layout Chunk
	n.chains = 1, n.iter = 10000, n.burnin = 2000, model.file = dlmFake, DIC =
 FALSE)
\end_layout

\begin_layout Chunk
out.mcmc <- as.mcmc(out)
\end_layout

\begin_layout Chunk
plot(1:nT, theta, type = 'l')
\end_layout

\begin_layout Chunk
lines(1:nT, colMeans(out.mcmc)[1:nT], col = 'blue')
\end_layout

\begin_layout Chunk
points(1:nT, y/n, col = 'red')
\end_layout

\begin_layout Chunk
plot(1:nT, theta, type = 'l')
\end_layout

\begin_layout Chunk
quants <- apply(out.mcmc[ , 1:nT], 2, quantile, c(.025, .975))
\end_layout

\begin_layout Chunk
# lines(1:nT, quants[1, ], col = 'blue', lty = 2)
\end_layout

\begin_layout Chunk
# lines(1:nT, quants[2, ], col = 'blue', lty = 2)
\end_layout

\begin_layout Chunk
polygon(cbind(c(1:nT, nT:1, 1), c(quants[1, ], rev(quants[2, ]), quants[1,
 1])), border = NA, col = 'lightblue')
\end_layout

\begin_layout Chunk
lines(1:nT, theta)
\end_layout

\begin_layout Chunk
lines(1:nT, colMeans(out.mcmc)[1:nT], col = 'blue')
\end_layout

\begin_layout Chunk
points(1:nT, y/n, col = 'red')
\end_layout

\begin_layout Chunk
plot(1:nrow(out.mcmc), out.mcmc[ , nT+1]) # tau
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Note: in this model, 
\begin_inset Formula $\tau^{2}$
\end_inset

 determines how quickly the process changes over time.
 Larger values allow more jumpiness while smaller values force smoother
 change.
\end_layout

\begin_layout Standard
Let's also see how we can choose our own starting values.
\end_layout

\begin_layout Chunk
<<fakeDLMstartingValues, eval=FALSE>>=
\end_layout

\begin_layout Chunk
inits <- function(){
\end_layout

\begin_layout Chunk
	list('tau' = runif(1, .01, 5), 'logitTheta' = rnorm(nT, 0, 6)) 
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk
# one can also specify inits as a list of fixed starting values
\end_layout

\begin_layout Chunk
# see the documentation for jags()
\end_layout

\begin_layout Chunk
out <- jags(data = list(nT = nT, n = n, y = y), parameters.to.save = c('tau',
 'theta'), 
\end_layout

\begin_layout Chunk
	n.chains = 3, n.iter = 5000, n.burnin = 1000, model.file = dlm, DIC = FALSE)
\end_layout

\begin_layout Chunk
traceplot(out)
\end_layout

\begin_layout Chunk
out.mcmc <- as.mcmc(out)
\end_layout

\begin_layout Chunk
par(mfrow = c(3, 1))
\end_layout

\begin_layout Chunk
for(i in 1:3)
\end_layout

\begin_layout Chunk
	plot(1:nrow(out.mcmc[[1]]), out.mcmc[[i]][ , nT+1], type = 'l') #tau
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Finally let's do an initial fit to some real data from Aino Pond in Massachusett
s.
 First we'll read in the data.
\end_layout

\begin_layout Chunk
<<readData, eval=FALSE>>=
\end_layout

\begin_layout Chunk
data <- read.csv('data/newEngl/PollenTimeSeries.csv')
\end_layout

\begin_layout Chunk
names(data)[4] <- "calAge"
\end_layout

\begin_layout Chunk
sub <- subset(data, sitename == 'aino')
\end_layout

\begin_layout Chunk
sub$calAge <- -sub$calAge
\end_layout

\begin_layout Chunk
sub <- sub[order(sub$calAge), ]
\end_layout

\begin_layout Chunk
sub$n <- rowSums(sub[ , 5:14])
\end_layout

\begin_layout Chunk
nT <- nrow(sub)
\end_layout

\begin_layout Chunk
plot(sub$calAge, sub$oak/sub$n)
\end_layout

\begin_layout Chunk
y <- sub$oak
\end_layout

\begin_layout Chunk
n <- round(sub$n)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Here's the BUGS code for the model.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

dlm <- function(){
\end_layout

\begin_layout Plain Layout

	logitTheta0 ~ dnorm(0, .000001)	
\end_layout

\begin_layout Plain Layout

	tau ~ dunif(0, 100)
\end_layout

\begin_layout Plain Layout

	tau2Inv <- 1/(tau^2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	logitTheta[1] ~ dnorm(logitTheta0, tau2Inv)
\end_layout

\begin_layout Plain Layout

    theta[1] <- exp(logitTheta[1])/(1+exp(logitTheta[1]))
\end_layout

\begin_layout Plain Layout

    y[1] ~ dbin(theta[1], n[1])
\end_layout

\begin_layout Plain Layout

	for(i in 2:nT){
\end_layout

\begin_layout Plain Layout

		logitTheta[i] ~ dnorm(logitTheta[i-1], tau2Inv)
\end_layout

\begin_layout Plain Layout

		theta[i] <- exp(logitTheta[i])/(1+exp(logitTheta[i]))
\end_layout

\begin_layout Plain Layout

		y[i] ~ dbin(theta[i], n[i])
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
And let's fit the model.
\end_layout

\begin_layout Chunk
<<fitModel, eval = FALSE>>=
\end_layout

\begin_layout Chunk
out <- jags(data = list(nT = nT, n = n, y = y), parameters.to.save = c('tau',
 'theta'), 
\end_layout

\begin_layout Chunk
	n.chains = 1, n.iter = 10000, n.burnin = 2000, model.file = dlm, DIC = FALSE)
\end_layout

\begin_layout Chunk
out.mcmc <- as.mcmc(out)
\end_layout

\begin_layout Chunk
thetaHat <- sub$oak/sub$n
\end_layout

\begin_layout Chunk
plot(sub$calAge, thetaHat, col = 'red')
\end_layout

\begin_layout Chunk
lines(sub$calAge, colMeans(out.mcmc)[1:nT], col = 'blue')
\end_layout

\begin_layout Chunk
quants <- apply(out.mcmc[ , 1:nT], 2, quantile, c(.025, .975))
\end_layout

\begin_layout Chunk
polygon(cbind(c(sub$calAge, rev(sub$calAge), sub$calAge[1]), c(quants[1,
 ], 
\end_layout

\begin_layout Chunk
	rev(quants[2, ]), quants[1, 1])), border = NA, col = 'lightblue')
\end_layout

\begin_layout Chunk
lines(sub$calAge, thetaHat, col = 'red')
\end_layout

\begin_layout Chunk
lines(sub$calAge, colMeans(out.mcmc)[1:nT], col = 'blue')
\end_layout

\begin_layout Chunk
title('oak at Aino Pond, MA')
\end_layout

\begin_layout Chunk
abline(v = 1650 - 1950, col = 'grey')
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Section
Next steps
\end_layout

\begin_layout Standard

\series bold
Challenge
\series default
: How we would code up a model that treats depth or radiocarbon years as
 the time index and correctly accounts for the irregular spacing of data.
\end_layout

\begin_layout Standard

\series bold
Challenge
\series default
: Let's consider some of the aspects of the data and the scientific context
 that have been left out of our model for pollen.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
overdispersion (add extra var?), discrete time, dmnorm with exponential
 covariance, categorical (multitaxa), should variance of RW not be homoscedastic
 on logit scale?, covariates in time evolution of process; demographic info
 in tiem evolution; smoothness of theta model; relate pollen to vegetation
\end_layout

\end_inset


\end_layout

\end_body
\end_document
